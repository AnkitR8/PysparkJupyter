{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001D0D796FFD0>\n",
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SparkSession is entry point to programming with dataset and dataframe API.\n",
    "#enableHiveSupport() enables hive suport including connectivity to a persistent hive metastore,\n",
    "    #support for hive serDes and hive user defined functions.\n",
    "#getOrCreate() clear by name.\n",
    "#from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "print(spark)\n",
    "\n",
    "#Once SparkSession is created, we can config spark's run time config properties like\n",
    "#spark.conf.set(\"spark.executor.memory\", \"2g\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|Ankit|100|\n",
      "|Rohan|200|\n",
      "+-----+---+\n",
      "\n",
      "+-----+-----+\n",
      "| name|marks|\n",
      "+-----+-----+\n",
      "|Ankit|  100|\n",
      "|Rohan|  200|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\opts\\spark\\spark-2.2.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py:336: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|marks| name|\n",
      "+-----+-----+\n",
      "|  100|Ankit|\n",
      "|  100|Ankit|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating dataframe from list\n",
    "lstOne=[('Ankit',100),('Rohan',200)]\n",
    "spark.createDataFrame(lstOne).show()\n",
    "\n",
    "from pyspark.sql import Row\n",
    "PersonSchema = Row('name','marks')\n",
    "rddTwo = rddOne.map(lambda x: PersonSchema(*x))\n",
    "spark.createDataFrame(rddTwo).show()\n",
    "#map can not be used with python list as map is spark functionality works with rdds.\n",
    "\n",
    "#Creating dataframe from a list of dictionary, we nee to have columnName:columnValue everytime\n",
    "#So, inferring schema from dictionay is deprecated, we should use pyspark.sql.row instead\n",
    "lstTwo=[{'name':'Ankit', 'marks':100},\n",
    "        {'name':'Ankit', 'marks':100}]\n",
    "spark.createDataFrame(lstTwo).show()\n",
    "\n",
    "#When schema is none, it will try to infer the schema (column names and data types) from data(can be Row, dict) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| name|marks|\n",
      "+-----+-----+\n",
      "|Ankit|  100|\n",
      "|Rohan|  200|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "| name|marks|\n",
      "+-----+-----+\n",
      "|Ankit|  100|\n",
      "|Rohan|  200|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now with schema - list of column names\n",
    "spark.createDataFrame(lstOne,['name','marks']).show()\n",
    "\n",
    "#Creating list to rdd then rdd to dataframe.\n",
    "rddOne = sc.parallelize(lstOne)\n",
    "spark.createDataFrame(rddOne, ['name','marks']).show()\n",
    "\n",
    "#when schema is a list of column names, the data type of each column will be inferred from data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySparkShell\n"
     ]
    }
   ],
   "source": [
    "print(sc.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| name|marks|\n",
      "+-----+-----+\n",
      "|Ankit|  100|\n",
      "|Rohan|  200|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"name\",StringType()),\n",
    "    StructField(\"marks\",IntegerType(), False)\n",
    "])\n",
    "#Third parameter False means the field is not nullable. By default is true.\n",
    "spark.createDataFrame(rddOne, schema).show()\n",
    "#When schema is pyspark.sql.types.Datatype or a data type string, it must match jthe real data, \n",
    "#or an exception will be thrown at the run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n",
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|   amlmiptrn|\n",
      "|     default|\n",
      "+------------+\n",
      "\n",
      "+-----+--------+---------+-----+\n",
      "|empId| empName|empSalary|depId|\n",
      "+-----+--------+---------+-----+\n",
      "|    1|   ankit|    10000|    4|\n",
      "|    2|  rajeev|    34000|    4|\n",
      "|    3|   kamal|    50000|    4|\n",
      "|    4|   vijay|   100000|    4|\n",
      "|    5| pradeep|    78999|    5|\n",
      "|    6|    aman|     8867|    5|\n",
      "|    7|  sanjay|    87826|    5|\n",
      "|    8|    raju|    74362|    5|\n",
      "|    9|   pawan|    78498|    5|\n",
      "|   10|  mayank|    78888|    2|\n",
      "|   11|shwetank|    54222|    2|\n",
      "|   12|himanshu|    89777|    2|\n",
      "|   13|  rajesh|    63455|    3|\n",
      "|   14|  suresh|    88787|    3|\n",
      "|   15|  vishal|    67676|    3|\n",
      "|   16|  apporv|    23456|    3|\n",
      "|   17|mahendra|    63423|    1|\n",
      "|   18| bhajnik|    87876|    1|\n",
      "|   19|  Shaily|    23534|    1|\n",
      "|   20|   baban|    76542|    1|\n",
      "+-----+--------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SET hive.execution.engine = spark')\n",
    "df=spark.sql('''select 'spark' as hello ''')\n",
    "df.show()\n",
    "\n",
    "spark.sql(\"show databases\").show()\n",
    "spark.sql(\"create database if not exists amlmiptrn\")\n",
    "spark.sql(\"use amlmiptrn\")\n",
    "#query to create table\n",
    "#spark.sql(\"create table emp(empId int, empName string, empSalary int, depId int) row format delimited fields terminated by ','\")\n",
    "\n",
    "df = spark.sql(\"select * from emp\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1,8,2).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
